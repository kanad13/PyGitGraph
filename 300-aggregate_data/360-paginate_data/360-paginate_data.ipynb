{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# purpose\n",
    "\n",
    "- as described [here](../readme.md), github graphql allows only 100 records to be fetched at a time\n",
    "- manual pagination is a lot of work\n",
    "- so this jupyter fully automates the extraction of data from github\n",
    "- each time you run this notebook, 100 records would be fetched from github and stored in a csv or json file in this folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structure\n",
    "\n",
    "- this scripts tracks\n",
    "  - what was the last record that was fetched\n",
    "  - from whereon should we start fetching the next set of records\n",
    "- this process is tracked by updating the file named [counter.json](./counter.json) - it is a python list with json objects within it\n",
    "- `counter.json` contains the [pageInfo](https://docs.github.com/en/graphql/reference/objects#pageinfo) details returned by each graphql query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# workflow\n",
    "\n",
    "- let us suppose you want to fetch details of all closed github issues till date\n",
    "- you start by emptying `counter.json` barring the first record\n",
    "- then execute the jupyter notebook\n",
    "- `if counter_value == 1:` validates if this is the first run\n",
    "- if it indeed is, then it will fetch the first 100 issues from your repository\n",
    "- and it would also increment the counter value in `counter.json`\n",
    "- counter.json will also be updated with the cursor value indicating which was the last record that was fetched\n",
    "- then the next time this notebook is run, the next 100 records would be fetched starting where the previous run ended\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch github token, python packages, queries and other parameters\n",
    "\n",
    "%run ../../100-set_parameters/100-set_parameters.ipynb # magic commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find counter_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read counter.json\n",
    "\n",
    "# load the counter.json\n",
    "def read_counter(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# fetch the current value of the counter by accessing the last json object in the list\n",
    "counter_file_path = 'counter.json'\n",
    "all_records = read_counter(counter_file_path)\n",
    "last_record = all_records[-1]\n",
    "counter_value = last_record[\"counter\"]\n",
    "\n",
    "# fetch also the current value of endCursor so that the next run can start from here\n",
    "end_cursor = last_record['endCursor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query parameters\n",
    "# specify the query parameters to be used while fetching issues\n",
    "# github does not allow fetching more than 100 issues\n",
    "\n",
    "fetch_issue_parameters = {\n",
    "    \"repository_name\": \"tensorflow\",\n",
    "    \"owner_name\": \"tensorflow\",\n",
    "    \"number_of_issues\": 100,\n",
    "\t\t\"end_cursor\": end_cursor\n",
    "  }\n",
    "\n",
    "with open('issue_params.json', 'w') as file:\n",
    "    json.dump(fetch_issue_parameters, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update counter_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update counter.json\n",
    "# the counter.json will be populated using the pageInfo details from the response to the latest query execution\n",
    "\n",
    "# load the input json that contains the extract graphql query\n",
    "def load_counter(input_file):\n",
    "    with open(input_file, 'r') as file:\n",
    "        input_data = json.load(file)\n",
    "    return input_data\n",
    "\n",
    "# write the page_info details inside the counter\n",
    "def write_json(page_info, output_file):\n",
    "\twith open(output_file,'r+') as file:\n",
    "\n",
    "              # first we load existing data into a dict\n",
    "              file_data = json.load(file)\n",
    "\n",
    "              # auto-increment counter\n",
    "              page_info['counter'] = len(file_data) + 1  # Auto-increment the counter\n",
    "\n",
    "              # append new_data with file_data inside run_counter\n",
    "              file_data.append(page_info)\n",
    "\n",
    "              # sets file's current position at offset\n",
    "              file.seek(0)\n",
    "\n",
    "              # convert back to json.\n",
    "              json.dump(file_data, file, indent = 4)\n",
    "\n",
    "def write_counter(input_file, output_file):\n",
    "\n",
    "    # get graphql response from the latest run\n",
    "    input_data = load_counter(input_file)\n",
    "\n",
    "    # extract pageInfo node from inside that response\n",
    "    page_info = input_data['data']['repository']['issues']['pageInfo']\n",
    "\n",
    "    # update counter.json so that the cursor value can be used for next run\n",
    "    write_json(page_info,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate data\n",
    "\n",
    "- previous functions help us to paginate through 100 records at a time\n",
    "- what that means is, i fetch the first 100 records and then insert them in a csv - plus i also update the counter\n",
    "- then i paginate through the next 100 records and insert them into a csv too\n",
    "- each time a new set of 100 records is received, that data is appended to [366-aggregated_data.csv](366-aggregated_data.csv)\n",
    "- this would be the final file that would be used for all visualizations [here](../../400-visualize_data/)\n",
    "- check this [readme](../readme.md) for details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append csv\n",
    "\n",
    "def append_csv_rows(input_data, output_data, include_headers):\n",
    "    # read rows from the source file\n",
    "    with open(input_data, 'r') as source:\n",
    "        reader = csv.reader(source)\n",
    "        rows = list(reader)  # convert the reader object to a list of rows\n",
    "\n",
    "    # do not include the column headers if they already exist in destination file\n",
    "    if not include_headers:\n",
    "        rows = rows[1:]  # exclude the first row (assumed to be headers)\n",
    "\n",
    "    # append rows to the destination file\n",
    "    with open(output_data, 'a', newline='') as destination:\n",
    "        writer = csv.writer(destination)\n",
    "        writer.writerows(rows)  # write the rows to the destination file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append json\n",
    "\n",
    "def append_json_data(source_file, aggregated_file):\n",
    "    # check if the aggregated file exists and read its data\n",
    "    if os.path.exists(aggregated_file):\n",
    "        with open(aggregated_file, 'r') as f:\n",
    "            aggregated_data = json.load(f)\n",
    "    else:\n",
    "        # initialize an empty structure if the file does not exist\n",
    "        aggregated_data = {\n",
    "            \"data\": {\n",
    "                \"viewer\": None,\n",
    "                \"repository\": {\n",
    "                    \"issues\": {\n",
    "                        \"edges\": []\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # load the new data to append\n",
    "    with open(source_file, 'r') as f:\n",
    "        new_data = json.load(f)\n",
    "\n",
    "    # initialize the viewer if it is not yet set\n",
    "    if aggregated_data['data']['viewer'] is None:\n",
    "        aggregated_data['data']['viewer'] = new_data['data']['viewer']\n",
    "\n",
    "    # append new issues to the aggregated data\n",
    "    new_issues = new_data['data']['repository']['issues']['edges']\n",
    "    aggregated_issues = aggregated_data['data']['repository']['issues']['edges']\n",
    "    aggregated_issues.extend(new_issues)\n",
    "\n",
    "    # write the updated data back to the aggregated file\n",
    "    with open(aggregated_file, 'w') as f:\n",
    "        json.dump(aggregated_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fetch data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query\n",
    "# decide to fetch first 100 records or next 100 records based on counter value captured in previous cells\n",
    "\n",
    "# the code would throw an error if there are less than 100 records to be fetched\n",
    "# the code still gets all the remaining records ... just throws an error - this is expected\n",
    "# so i have just suppressed the error\n",
    "\n",
    "try:\n",
    "    if counter_value == 1:\n",
    "        print(\"getting first 100 records\")\n",
    "\n",
    "        # fetch first 100 records\n",
    "        %run ./../320-fetch_first_100_closed_issues/320-fetch_first_100_closed_issues.ipynb\n",
    "\n",
    "        # update counter\n",
    "        input_file = '320-first_100_closed_issues.json'\n",
    "        output_file = 'counter.json'\n",
    "        write_counter(input_file, output_file)\n",
    "\n",
    "        # append data to CSV\n",
    "        input_data_csv = '320-first_100_closed_issues.csv'\n",
    "        output_data_csv = '366-aggregated_data.csv'\n",
    "        include_headers = True\n",
    "        append_csv_rows(input_data_csv, output_data_csv, include_headers)\n",
    "\n",
    "        # append data to JSON\n",
    "        input_data_json = '320-first_100_closed_issues.json'\n",
    "        output_data_json = '366-aggregated_data.json'\n",
    "        append_json_data(input_data_json, output_data_json)\n",
    "\n",
    "    else:\n",
    "        print(\"getting next 100 records\")\n",
    "\n",
    "        # fetch next 100 records\n",
    "        %run ./../340-fetch_next_100_closed_issues/340-fetch_next_100_closed_issues.ipynb\n",
    "\n",
    "        # update counter\n",
    "        input_file = '340-next_100_closed_issues.json'\n",
    "        output_file = 'counter.json'\n",
    "        write_counter(input_file, output_file)\n",
    "\n",
    "        # append data to CSV\n",
    "        input_data_csv = '340-next_100_closed_issues.csv'\n",
    "        output_data_csv = '366-aggregated_data.csv'\n",
    "        include_headers = False\n",
    "        append_csv_rows(input_data_csv, output_data_csv, include_headers)\n",
    "\n",
    "        # append data to JSON\n",
    "        input_data_json = '340-next_100_closed_issues.json'\n",
    "        output_data_json = '366-aggregated_data.json'\n",
    "        append_json_data(input_data_json, output_data_json)\n",
    "\n",
    "except Exception:\n",
    "    # handle the exception if necessary in future\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
